---
title: 'From Fiction to Reality\: As Hate Speech Spread So Does Hate Crime? A Twitter
  Investigation'
author: "Lennart Roesemeier"
date: "July 7, 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
    fig_caption: yes
fontsize: 12pt
---

```{r include=FALSE, warning=FALSE, results="hide", message=FALSE}
#loading required packages, it downloads a package if it isn't downloaded yet
if (!require(RCurl)) install.packages("RCurl", dependencies = T)
library(RCurl)
if (!require(curl)) install.packages("curl", dependencies = T)
library(curl)
if (!require(tidyverse)) install.packages("tidyverse", dependencies = T)
library(tidyverse)
if (!require(tidytext)) install.packages("tidytext", dependencies = T)
library(tidytext)
if (!require(stringi)) install.packages("stringi", dependencies = T)
library(stringi)
if (!require(quanteda)) install.packages("quanteda", dependencies = T)
library(quanteda)
if (!require(tm)) install.packages("tm", dependencies = T)
library(tm)
if (!require(data.table)) install.packages("data.table", dependencies = T)
library(data.table)
if (!require(tokenizers)) install.packages("tokenizers", dependencies = T)
library(tokenizers)
if (!require(magrittr)) install.packages("magrittr", dependencies = T)
library(magrittr)
if (!require(anytime)) install.packages("anytime", dependencies = T)
library(anytime)
if (!require(rgdal)) install.packages("rgdal", dependencies = T)
library(rgdal)
if (!require(sp)) install.packages("sp")
library(sp)
if (!require(mapproj)) install.packages("mapproj")
library(mapproj)
```




```{r include=FALSE, results="hide", message=FALSE, warning=FALSE}
# loading Data
# incident data provided by Mueller and Schwarz (2019)
inc <- getURL("https://raw.githubusercontent.com/ax3l/chronik-vorfaelle/data/vorfaelle.csv")
incidents <- read.csv(text = inc)

# loading conntrol variables
c <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/control_variables.csv")
control <- read.csv(text = c)

# first scrape of the keyword "Fluechtling(e)" (refugee(s))
first <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_first.csv")
tweets_first <- read.csv(text = first)

# second scrape of "Fluechtling(e)" (refugee(s))
fl <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_f.csv") 
fl_tweets <- read.csv(text = fl)

# scrape of the keyword "asylant" (asylee(s))
asy <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_a.csv")
asy_tweets <- read.csv(text = asy)

# scrape of the keyword "migrant(en)" (migrant(s))
mi <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_m.csv")
mi_tweets <- read.csv(text = mi)

# remove the URLs
rm(inc)
rm(c)
rm(first)
rm(fl)
rm(asy)
rm(mi)

# compute the difference between the first and the second scrape process
diff_firstsecond_scrape <- tweets_first %>%
  anti_join(fl_tweets, by = "text")

# binding all tweets together
tweets <- fl_tweets %>%
  bind_rows(diff_firstsecond_scrape) %>%
  bind_rows(asy_tweets) %>%
  bind_rows(mi_tweets)

# remove redundant dataframes of single scrape waves
rm(tweets_first)
rm(fl_tweets)
rm(asy_tweets)
rm(mi_tweets)
rm(diff_firstsecond_scrape)
```

```{r}
# encode all tweets to utf-8
tweets$text <- stri_encode(tweets$text, "", "UTF-8")

# transform tweets to a corpus to remove URLs within tweets
tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))

#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))

# removing URLs
corpus <- tm_map(corpus, removeURL)

# transfomr corpus to dataframe and replace the tweets within the tweet data
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)
tweets$text <- d$text

# remove redundant data
rm(d)
rm(tweetdata)
rm(corpus)
```

```{r}
# transform to tibble to work the tidyverse package
tweets <- as_tibble(tweets)

# transform factors to characters, easier to handle
tweets %>%
  mutate_if(is.factor, as.character) 

# transform umlauts and remove all non-alphanumerical characters
tweets$text <- str_replace_all(tweets$text,
                c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss",
                  "Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue",
                  "Â¤" = "ae", "Â¶" = "oe", "Â¼" = "ue", "ÃY" = "ss", 
                  'Ã"' = "Ae", "Ã-" = "Oe", "Ão" = "Ue", "[\r\n]" = " ", 
                  "[[:punct:]]" = " " , "[^[:alnum:] ]" = " ", 
                  "[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " "))

tweets$text <- tolower(tweets$text)
```

```{r}
# delete non german or englisch tweets during removing unicode specific tweets 
# (contains \s + hexdecimal numbers)
# which cannot be transformed to utf-8
setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]

empty_char <- tweets %>%
  filter(!str_detect(text, ""))

tweets <- tweets %>%
  anti_join(empty_char, by = "text")

rm(empty_char)
```



```{r}
# loading stopwords and transform them equally to the tweet transformation
stopwords <- data_frame(word = stopwords("de"))
stopwords <- stopwords %>%
  data_frame(str_replace_all(stopwords$word,
                c("Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue", "ß" = "ss", 
                  "Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
  set_colnames(c("word1", "word2")) %>%
  select("word2") %>%
  rename("word" = "word2")

# due to the fact that the loaded german stopwords doesn't contain web expressions like "wtf", "lol", etc. 
# these expression are added manually to the stopword list
c_stop <- data.frame(word = c("via", "mehr", "twitter", "com", "pic", "de", "eu", "d", "kommen", "mal", 
                                       "sollen", "news", "viele", "ja", "online", "geht", "heute", "schon", "neue", 
                                       "u", "immer", "muessen", "wer", "gibt", "s", "ab", "rt", "gut", "macht", 
                                       "statt", "a", "n", "nimmt", "sagt", "seit", "unsere", "fast", "sz", "f", 
                                       "brauchen", "beim", "nehmen", "neue", "bitte", "gerade", "zurueck", "is", 
                                       "lassen", "dafuer", "einfach", "wegen", "f", "m", "kommt", "denen", "re",
                                       "duerfen", "i", "wohl", "waere", "weitere", "b", "gehen", "gar", "erst", "erste",
                                       "sollten", "wirklich", "h", "sagen", "sieht", "bringen", "eigentlich", 
                                       "suchen", "gg", "o", "tun", "ganz", "stellt", "e", "laesst", "w", "bald", 
                                       "br", "faz", "faznet", "at", "geben", "ndr", "no", "t", "z", "all", "for", 
                                       "g", "davon", "haette", "wurde", "setzt", "the", "genau", "k", "r", "gleich", 
                                       "kamen", "c", "koennten", "l", "waeren", "ntvde", "vielleicht", "dabei", 
                                       "darf", "haelt", "offenbar", "wenig", "allein", "kaum", "orfsg", "by", "klar", 
                                       "wg", "bislang", "na", "neuen", "eignen", "heisst", "leider", "letzte", 
                                       "paar", "ots", "womoeglich", "add", "etc", "ne", "sogar", "trotz", "schnell", 
                                       "wdr", "jemand", "tolle", "wieso", "boah", "ganze", "halten", "rp", "darum",
                                       "hh", "toll", "top", "x", "bereits", "etwa", "schoen", "taz", "domain", "her",
                                       "j", "mdr", "orban", "p", "sv", "tt", "wollten", "ex", "focusonline", "live",
                                       "nix", "nzz", "of", "tut", "ca", "ikea", "mrd", "unsere", "alte", "daran", 
                                       "darueber", "deutlich", "eher", "ha", "je", "laedt", "laesst", "moechte", 
                                       "stimmt", "ueberall", "usw", "telmi", "zib", "ach", "and", "eben", "en", 
                                       "natuerlich", "to", "ttip", "alter", "egal", "fb", "fc", "heutejournal", 
                                       "oft", "sternde", "vs", "aktuelle" ,"au", "bisserl", "gerne", "grosses", 
                                       "guter", "here", "magazin", "oh", "sant", "st", "till", "srfarena", "deren", 
                                       "ey", "gab", "ganzen", "haetten", "heuteshow", "per", "wann", "ardde", "deai", 
                                       "dr", "hast", "kurz", "on", "rponline", "somit", "telekom", "tonline", "unseren", 
                                       "zusammen", "art", "ghadajreidi", "hna", "mag", "mo", "oevp", "sc", "selber", 
                                       "whataretheodds", "weder", "wieviele", "worden", "y", "be", "bplus", "gilt", 
                                       "it", "kam", "seid", "serie", "steve", "ts", "uebringens", "unseren", "wooow", 
                                       "binnen", "darauf", "daserste", "dwn", "fl", "fpoe", "free", "habt", "los", "mass", 
                                       "nahm", "newsrepublicde", "oe", "sst", "staendig", "weist", "weit", "wtf", 
                                       "badlands", "bvbqfk", "deut", "drk", "dt", "eh", "hallo", "halt", "lt", "new", 
                                       "q", "sowas", "sowie", "spaeter", "web", "aotto", "bitten", "echt", "eigene", 
                                       "extra", "meint", "obwohl", "orf", "sei", "uro", "wen", "zb", "apa", "ber", "caf", 
                                       "card", "ch", "co", "gegenueber", "gern", "iv", "kleine", "more", "mt", "naechste", 
                                       "naemlich", "netzplanet", "se", "stream", "tops", "us", "waer", "we", "you", 
                                       "abendblatt", "aeh", "amepres", "cloud", "del", "deshalb", "gibts", "info", 
                                       "inzwischen", "irgendwie", "kennen", "klare", "nd", "palabra", "ohv", "schoene", 
                                       "server", "seien", "sog", "soviel", "svp", "swiss", "tolles", "zdfheute", "aha", 
                                       "al", "blzonline", "bot", "dji", "flue", "ger", "guten", "gutes", "jo", "me", 
                                       "migazin", "neben", "nen", "phoenix", "sn", "vds"))

c_stopwords <- c_stop %>%
  as_tibble() %>%
  bind_rows(stopwords)

# remove redundant data
rm(c_stop)
rm(stopwords)

# create token list 1755 different tokens
tweetwords <- tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(c_stopwords, by = "word") %>%
  count(word,  sort=T) %>%
  filter(n > 10) #more or less clean dataset with 1503 tokens
```


```{r}
# load sentiment tables from the Wortschatu by the University of Leipzig
sentiments <- bind_rows(
  read.table("SentiWS_v2.0_Negative.txt", sep = "\t", fill = T),
  read.table("SentiWS_v2.0_Positive.txt", sep = "\t", fill = T)) %>%
  select(.,1:2)

# add keywords because they aren't in the sentiment tables, 
# except "fluechtling" which has the sentiment value of -0.0048
# due to that I assign this value to the other keywords as well
f_pl <- data.frame(word = c("fluechtlinge", "migrant", "migranten", 
                            "asylant", "asylanten"), 
                   value = -0.0048)

# sentiments are transformed just as the tweets
# and a new dataset is created with expression which occurs in both (sentiment/tweet) datasets
sentis <- sentiments %>%
  mutate(word = str_replace_all(sentiments$V1,
                c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss", 
                  "Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
  rename("value" = "V2") %>%
  select(value, word) %>%
  mutate(word = str_to_lower(word)) %>%
  mutate(word = gsub("\\|nn", "", word)) %>%
  bind_rows(f_pl) %>%
  inner_join(tweetwords, by = "word") %>%
  mutate(sentiment = ifelse(value >= 0, "positive", "negative")) %>%
  rename("freq" = "n") %>%
  arrange(desc(freq))

# remove redundant data
rm(f_pl)
rm(sentiments)

# calculate the overall positve and negative sentiments
sentis %>%
  group_by(sentiment) %>%
  count()
```

```{r echo=FALSE, fig.height=5, fig.width=14, warning=FALSE}
# show the distribution of positive/negative seniments, proper
sentis %>%
  ggplot(aes(word, value, fill = sentiment, label = word)) +
  geom_col() +
  labs(title="Positive/Negative Sentiments of the Tweets", 
       y = "", x= "Tokens")  +
  coord_cartesian(ylim = c(-1,1)) +
  theme_minimal() +
  theme(axis.title.x = element_text(),
        axis.text.x = element_text(angle = 90),
        axis.ticks.x = element_blank(),
        axis.title.y = element_text(),
        axis.text.y = element_text(),
        axis.ticks.y = element_blank()) +
  guides(fill = guide_legend(title = "Sentiment"))
  
```



```{r}
incidents$features_properties_date <- as.character(incidents$features_properties_date)
incidents$features_properties_date <- as.Date(incidents$features_properties_date, "%d.%m.%Y")

incidents <- incidents %>%
  filter(features_properties_date >= "2015-01-01" & features_properties_date < "2016-01-01")
```

```{r}
graph <- incidents %>%
  select(features_properties_city, features_properties_date, features_properties_state) %>%
  rename("city" = "features_properties_city", "date" = "features_properties_date", "state" = "features_properties_state") %>%
  group_by(state) %>%
  count()

ger <- readRDS("DEU_adm1.rds")
states <- fortify(ger)
states$state <- factor(as.numeric(states$id))
levels(states$state) <- ger$NAME_1
state_inc <- merge(states, graph)

ggplot(state_inc, aes(x = long, y = lat, group = group, fill = n)) +
    geom_polygon(col = "white") +
    coord_map() +
    theme_void() +
    theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(title = "Incidents against Refugees in the Year 2015", y = "", x = "", fill = "Number of Incidents")
```


```{r}
rm(ger)
rm(states)
rm(state_inc)

control$state <- str_replace_all(control$state,
                                 c("ue" = "ü"))

incidents <- incidents %>%
  select(features_properties_date, features_properties_state, features_properties_title) %>%
  rename("date" = "features_properties_date", "state" = "features_properties_state", "title" = "features_properties_title") %>%
  full_join(control, by = "state")

tweets$timestamp <- as.Date(tweets$timestamp, "%Y-%m-%d %H:%M:%S")

tweets <- tweets %>%
  select(tweet_id, timestamp, text) %>%
  rename("date" = "timestamp")
```


```{r}
sum_tweets <- tweets %>%
  group_by(date) %>%
  count() %>%
  rename("tweets" = "n") 

sum_tweets[, 2] <- log(sum_tweets[2])

sum_inc <- incidents %>%
  group_by(date) %>%
  count() %>%
  rename("num_inc" = "n")

sum_inc[, 2] <- log(sum_inc[2])

all_sum <- sum_tweets %>%
  full_join(sum_inc, by = "date") %>%
  replace_na(list(tweets = 0, num_inc = 0))

all_sum_wm <- sum_tweets %>%
  inner_join(sum_inc, by = "date")
```



```{r}
all_sum %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = tweets, colour = "tweets")) +
  geom_line(aes(y = num_inc, colour = "num_inc")) +
  labs(title = "Logarithmized: Hate Crime and Tweets with the word 'refugee'", x = "Date", y = "", colour = "Legend") +
  scale_color_manual(labels = c("Number of Incidents", "Tweets"), values = c("#009E73", "#E69F00")) +
  theme_minimal()
```


```{r}
all_sum_wm %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = tweets, colour = "tweets")) +
  geom_line(aes(y = num_inc, colour = "num_inc")) +
  labs(title = "Logarithmized: Hate Crime and Tweets with the word 'refugee'", x = "Date", y = "", colour = "Legend") +
  scale_color_manual(labels = c("Number of Incidents", "Tweets"), values = c("#009E73", "#E69F00")) +
  theme_minimal()
```


```{r}
sum_t1 <- tweets %>%
  group_by(date) %>%
  count() %>%
  rename("tweets" = "n")

all <- incidents %>%
  group_by(date, state) %>%
  count() %>%
  rename("num_inc" = "n") %>%
  full_join(sum_t1, by = "date") %>%
  full_join(control, by = "state") %>%
  replace_na(list(tweets = 0))
```


```{r}
inc_dnk <- all %>%
  filter(date >= "2015-02-07" & date <= "2015-02-21") %>%
  na.omit()

inc_dnk$time <- ifelse(inc_dnk$date > "2015-02-14", 1, 0)
inc_dnk$did <- ifelse(inc_dnk$date > "2015-02-14", 31, 0)

dnk <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_dnk)
summary(dnk)
```

```{r}
inc_usa1 <- all %>%
  filter(date >= "2015-06-10" & date <= "2015-06-24") %>%
  na.omit()

inc_usa1$time <- ifelse(inc_usa1$date > "2015-06-17", 1, 0)
inc_usa1$did <- ifelse(inc_usa1$date > "2015-06-17", 0, 809)

usa1 <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_usa1)
summary(usa1)
```

```{r}
inc_usa2 <- all %>%
  filter(date >= "2015-11-25" & date <= "2015-12-09") %>%
  na.omit()

inc_usa2$time <- ifelse(inc_usa2$date > "2015-12-02", 1, 0)
inc_usa2$did <- ifelse(inc_usa2$date > "2015-12-02", inc_usa2$time * inc_usa2$tweets, 0)


usa2 <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_usa2)
summary(usa2)
```

```{r}
inc_fra1 <- all %>%
  filter(date >= "2015-06-19" & date <= "2015-07-03")

inc_fra1$time <- ifelse(inc_fra1$date > "2015-06-26", 1, 0)
inc_fra1$did <- ifelse(inc_fra1$date > "2015-06-26", 784, 0)

fra1 <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra1)
summary(fra1)
```

```{r}
inc_deu <- all %>%
  filter(date >= "2015-09-10" & date <= "2015-09-24")

inc_deu$time <- ifelse(inc_deu$date > "2015-09-17", 1, 0)
inc_deu$did <- ifelse(inc_deu$date > "2015-09-17", 857, 0)

deu <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_deu)
summary(deu)
```

```{r}
inc_aus <- all %>%
  filter(date >= "2015-09-24" & date <= "2015-10-09")

inc_aus$time <- ifelse(inc_aus$date > "2015-10-02", 1, 0)
inc_aus$did <- ifelse(inc_aus$date > "2015-10-02", 862, 0)

aus <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_aus)
summary(aus)
```





```{r}
inc_fra2 <- all %>%
  filter(date >= "2015-11-06" & date <= "2015-11-20")

inc_fra2$time <- ifelse(inc_fra2$date > "2015-11-13", 1, 0)
inc_fra2$did <- ifelse(inc_fra2$date > "2015-11-13", inc_fra2$time * inc_fra2$tweets, 0)

fra2 <- lm(num_inc ~ tweets + did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra2)
summary(fra2)
```

```{r}
inc_dnk_wt <- all %>%
  filter(date >= "2015-02-07" & date <= "2015-02-21") %>%
  na.omit()

inc_dnk_wt$time <- ifelse(inc_dnk_wt$date > "2015-02-14", 1, 0)
inc_dnk_wt$treated <- ifelse(inc_dnk_wt$date > "2015-02-14", 1, 0)
inc_dnk_wt$did <- ifelse(inc_dnk_wt$date > "2015-02-14", inc_dnk_wt$time*inc_dnk_wt$treated, 0)

dnk_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_dnk_wt)
summary(dnk_wt)
```

```{r}
inc_usa1_wt <- all %>%
  filter(date >= "2015-06-10" & date <= "2015-06-24")

inc_usa1_wt$time <- ifelse(inc_usa1_wt$date > "2015-06-17", 1, 0)
inc_usa1_wt$treated <- ifelse(inc_usa1_wt$date > "2015-06-17", 
                             ifelse(inc_usa1_wt$num_inc < 1, 0, 1), 0)
inc_usa1_wt$did <- ifelse(inc_usa1_wt$date > "2015-06-17", inc_usa1_wt$time*inc_usa1_wt$num_inc, 0)

usa1_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_usa1_wt)
summary(usa1_wt)
```

```{r}
inc_usa2_wt <- all %>%
  filter(date >= "2015-11-25" & date <= "2015-12-09")

inc_usa2_wt$time <- ifelse(inc_usa2_wt$date > "2015-12-02", 1, 0)
inc_usa2_wt$treated <- ifelse(inc_usa2_wt$date > "2015-12-02", 
                             ifelse(inc_usa2_wt$num_inc < 1, 0, 1), 0)
inc_usa2_wt$did <- ifelse(inc_usa2_wt$date > "2015-12-02", inc_usa2_wt$time * inc_usa2_wt$num_inc, 0)


usa2_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_usa2_wt)
summary(usa2_wt)
```

```{r}
inc_fra1_wt <- all %>%
  filter(date >= "2015-06-19" & date <= "2015-07-03")

inc_fra1_wt$time <- ifelse(inc_fra1_wt$date > "2015-06-26", 1, 0)
inc_fra1_wt$treated <- ifelse(inc_fra1_wt$date > "2015-06-26", 
                             ifelse(inc_fra1_wt$num_inc < 1, 0, 1), 0)
inc_fra1_wt$did <- ifelse(inc_fra1_wt$date > "2015-06-26", inc_fra1_wt$num_inc*inc_fra1_wt$time, 0)

fra1_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra1_wt)
summary(fra1_wt)
```

```{r}
inc_deu_wt <- all %>%
  filter(date >= "2015-09-10" & date <= "2015-09-24")

inc_deu_wt$time <- ifelse(inc_deu_wt$date > "2015-09-17", 1, 0)
inc_deu_wt$treated <- ifelse(inc_deu_wt$date > "2015-09-17", 
                             ifelse(inc_deu_wt$num_inc < 1, 0, 1), 0)
inc_deu_wt$did <- ifelse(inc_deu_wt$date > "2015-09-17", inc_deu_wt$num_inc*inc_deu_wt$time, 0)

deu_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_deu_wt)
summary(deu_wt)
```

```{r}
inc_aus_wt <- all %>%
  filter(date >= "2015-09-24" & date <= "2015-10-09")

inc_aus_wt$time <- ifelse(inc_aus_wt$date > "2015-10-02", 1, 0)
inc_aus_wt$treated <- ifelse(inc_aus_wt$date > "2015-10-02", 
                             ifelse(inc_aus_wt$num_inc < 1, 0, 1), 0)
inc_aus_wt$did <- ifelse(inc_aus_wt$date > "2015-10-02", inc_aus_wt$num_inc*inc_aus_wt$time, 0)

aus_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_aus_wt)
summary(aus_wt)
```

```{r}
inc_fra2_wt <- all %>%
  filter(date >= "2015-11-06" & date <= "2015-11-20")

inc_fra2_wt$time <- ifelse(inc_fra2_wt$date > "2015-11-13", 1, 0)
inc_fra2_wt$treated <- ifelse(inc_fra2_wt$date > "2015-11-13", 
                             ifelse(inc_fra2_wt$num_inc < 1, 0, 1), 0)
inc_fra2_wt$did <- ifelse(inc_fra2_wt$date > "2015-11-13", inc_fra2_wt$time*inc_fra2_wt$time, 0)

fra2_wt <- lm(num_inc ~ did + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra2_wt)
summary(fra2_wt)
```






