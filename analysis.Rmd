---
title: 'From Fiction to Reality\: As Hate Speech Spread So Does Hate Crime? A Twitter
  Investigation'
author: "Lennart Roesemeier"
date: "July 7, 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
    fig_caption: yes
fontsize: 12pt
---

```{r include=FALSE, warning=FALSE, results="hide", message=FALSE}
#loading required packages, it installs a package if it isn't installed yet
if (!require(RCurl)) install.packages("RCurl", dependencies = T)
library(RCurl)
if (!require(curl)) install.packages("curl", dependencies = T)
library(curl)
if (!require(tidyverse)) install.packages("tidyverse", dependencies = T)
library(tidyverse)
if (!require(tidytext)) install.packages("tidytext", dependencies = T)
library(tidytext)
if (!require(stringi)) install.packages("stringi", dependencies = T)
library(stringi)
if (!require(quanteda)) install.packages("quanteda", dependencies = T)
library(quanteda)
if (!require(tm)) install.packages("tm", dependencies = T)
library(tm)
if (!require(data.table)) install.packages("data.table", dependencies = T)
library(data.table)
if (!require(tokenizers)) install.packages("tokenizers", dependencies = T)
library(tokenizers)
if (!require(magrittr)) install.packages("magrittr", dependencies = T)
library(magrittr)
if (!require(rgdal)) install.packages("rgdal", dependencies = T)
library(rgdal)
if (!require(sp)) install.packages("sp")
library(sp)
if (!require(mapproj)) install.packages("mapproj")
library(mapproj)
if (!require(stargazer)) install.packages("stargazer")
library(stargazer)
if (! require(pastecs)) install.packages("pastecs")
library(pastecs)
if (! require(xtable)) install.packages("xtable")
library(xtable)
```


```{r include=FALSE, results="hide", message=FALSE, warning=FALSE}
# !!!!!!!!!!!!!!!!!!!Attention!!!!!!!!!!!!!!!!!!!   loading may take some time (~10min, depends obviously on broadband access and available kernel)
# loading Data
# incident data provided by Mueller and Schwarz (2019)
inc <- getURL("https://raw.githubusercontent.com/ax3l/chronik-vorfaelle/data/vorfaelle.csv")
incidents <- read.csv(text = inc)

# loading conntrol variables
c <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/control_variables.csv")
control <- read.csv(text = c)

# scrape wave 1 = attack on paris, france in january (7th)
w1 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs1_jan_de.csv")
w1_tweets <- read.csv(text = w1)

# loading scraped tweets, wave 2 = attack on copenhagen, denmark in february (14th)
w2 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs2_feb_de.csv")
w2_tweets <- read.csv(text = w2)

#loading scraped tweets, wave 3 = attack on charleston, USA in june (17th)
w3 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs3_jun_de.csv")
w3_tweets <- read.csv(text = w3)

# loading scraped tweets, wave 4 = attack on Saint Quentin-Fallavier, France in June (26th)
w4 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs4_jun_de.csv")
w4_tweets <- read.csv(text = w4)

# loading scraped tweets, wave 5 = attack on Bielefeld, Germany in september (9th)
w5 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs5_sep_de.csv")
w5_tweets <- read.csv(text = w5)

# loading scraped tweets, wave 6 = attack on Berlin, Germany in September (17th)
w6 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs6_sep_de.csv")
w6_tweets <- read.csv(text = w6)

# loading scraped tweets, wave 7 = attack on Parramatta City, Australia in october (2nd)
w7 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs7_oct_de.csv")
w7_tweets <- read.csv(text = w7)

# loading scraped tweets, wave 8 = attack on Paris, France in november (13th)
w8 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs8_nov_de.csv")
w8_tweets <- read.csv(text = w8)

# loading scraped tweets, wave 9 = attack on San Bernardino, USA in december (2nd)
w9 <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_hs9_dec_de.csv")
w9_tweets <- read.csv(text = w9)

# remove the URLs
rm(inc, c, w1, w2, w3, w4, w5, w6, w7, w8, w9)

# binding all tweets together
tweets <- w1_tweets %>%
  bind_rows(w2_tweets, w3_tweets, w4_tweets, w5_tweets, w6_tweets, w7_tweets, w8_tweets, w9_tweets) %>% # 21591
  distinct()  # 17557, double tweets: 4034

# remove redundant dataframes of single scrape waves
rm(w1_tweets, w2_tweets, w3_tweets, w4_tweets, w5_tweets, w6_tweets, w7_tweets, w8_tweets, w9_tweets)

incidents %>%
  group_by(features_properties_title) %>%
  count()
```


```{r}
# encode all tweets to utf-8
tweets$text <- stri_encode(tweets$text, "", "UTF-8")

# transform tweets to a corpus to remove URLs within tweets
tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))

#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) #lock at the unicode!! must fix S+

# removing URLs
corpus <- tm_map(corpus, removeURL)

# transfomr corpus to dataframe and replace the tweets within the tweet data
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)
tweets$text <- d$text

# remove redundant data
rm(d)
rm(tweetdata)
rm(corpus)
```


```{r}
# transform to tibble to work the tidyverse package
tweets <- as_tibble(tweets)

# transform factors to characters, easier to handle
tweets %>%
  mutate_if(is.factor, as.character) 

# transform umlauts and remove all non-alphanumerical characters
tweets$text <- str_replace_all(tweets$text,
                c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss",
                  "Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue",
                  "Â¤" = "ae", "Â¶" = "oe", "Â¼" = "ue", "ÃY" = "ss", 
                  'Ã"' = "Ae", "Ã-" = "Oe", "Ão" = "Ue", "[\r\n]" = " ", 
                  "[[:punct:]]" = " " , "[^[:alnum:] ]" = " ", 
                  "[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " "))

tweets$text <- tolower(tweets$text)
```


```{r}
# delete non german or englisch tweets during removing unicode specific tweets 
# (contains \s + hexdecimal numbers)
# which cannot be transformed to utf-8
setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]

empty_char <- tweets %>%
  filter(!str_detect(text, ""))

tweets <- tweets %>%
  anti_join(empty_char, by = "text")

rm(empty_char)

# calculate individual accounts
tweets %>%
  select(screen_name) %>%
  group_by(screen_name) %>%
  count() %>%
  mutate(sum = n-n+1) %>%
  ungroup() %>%
  count(sum) #8117
```


```{r}
# loading stopwords and transform them equally to the tweet transformation
stopwords <- data_frame(word = stopwords("de"))
stopwords <- stopwords %>%
  data_frame(str_replace_all(stopwords$word,
                c("Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue", "ß" = "ss", 
                  "Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
  set_colnames(c("word1", "word2")) %>%
  select("word2") %>%
  rename("word" = "word2")

# due to the fact that the loaded german stopwords doesn't contain web expressions like "wtf", "lol", etc. 
# these expression are added manually to the stopword list
c_stop <- data.frame(word = c("via", "mehr", "twitter", "com", "pic", "de", "eu", "d", "kommen", "mal", 
                                       "sollen", "news", "viele", "ja", "online", "geht", "heute", "schon", "neue", 
                                       "u", "immer", "muessen", "wer", "gibt", "s", "ab", "rt", "gut", "macht", 
                                       "statt", "a", "n", "nimmt", "sagt", "seit", "unsere", "fast", "sz", "f", 
                                       "brauchen", "beim", "nehmen", "neue", "bitte", "gerade", "zurueck", "is", 
                                       "lassen", "dafuer", "einfach", "wegen", "f", "m", "kommt", "denen", "re",
                                       "duerfen", "i", "wohl", "waere", "weitere", "b", "gehen", "gar", "erst", "erste",
                                       "sollten", "wirklich", "h", "sagen", "sieht", "bringen", "eigentlich", 
                                       "suchen", "gg", "o", "tun", "ganz", "stellt", "e", "laesst", "w", "bald", 
                                       "br", "faz", "faznet", "at", "geben", "ndr", "no", "t", "z", "all", "for", 
                                       "g", "davon", "haette", "wurde", "setzt", "the", "genau", "k", "r", "gleich", 
                                       "kamen", "c", "koennten", "l", "waeren", "ntvde", "vielleicht", "dabei", 
                                       "darf", "haelt", "offenbar", "wenig", "allein", "kaum", "orfsg", "by", "klar", 
                                       "wg", "bislang", "na", "neuen", "eignen", "heisst", "leider", "letzte", 
                                       "paar", "ots", "womoeglich", "add", "etc", "ne", "sogar", "trotz", "schnell", 
                                       "wdr", "jemand", "tolle", "wieso", "boah", "ganze", "halten", "rp", "darum",
                                       "hh", "toll", "top", "x", "bereits", "etwa", "schoen", "taz", "domain", "her",
                                       "j", "mdr", "orban", "p", "sv", "tt", "wollten", "ex", "focusonline", "live",
                                       "nix", "nzz", "of", "tut", "ca", "ikea", "mrd", "unsere", "alte", "daran", 
                                       "darueber", "deutlich", "eher", "ha", "je", "laedt", "laesst", "moechte", 
                                       "stimmt", "ueberall", "usw", "telmi", "zib", "ach", "and", "eben", "en", 
                                       "natuerlich", "to", "ttip", "alter", "egal", "fb", "fc", "heutejournal", 
                                       "oft", "sternde", "vs", "aktuelle" ,"au", "bisserl", "gerne", "grosses", 
                                       "guter", "here", "magazin", "oh", "sant", "st", "till", "srfarena", "deren", 
                                       "ey", "gab", "ganzen", "haetten", "heuteshow", "per", "wann", "ardde", "deai", 
                                       "dr", "hast", "kurz", "on", "rponline", "somit", "telekom", "tonline", "unseren", 
                                       "zusammen", "art", "ghadajreidi", "hna", "mag", "mo", "oevp", "sc", "selber", 
                                       "whataretheodds", "weder", "wieviele", "worden", "y", "be", "bplus", "gilt", 
                                       "it", "kam", "seid", "serie", "steve", "ts", "uebringens", "unseren", "wooow", 
                                       "binnen", "darauf", "daserste", "dwn", "fl", "fpoe", "free", "habt", "los", "mass", 
                                       "nahm", "newsrepublicde", "oe", "sst", "staendig", "weist", "weit", "wtf", 
                                       "badlands", "bvbqfk", "deut", "drk", "dt", "eh", "hallo", "halt", "lt", "new", 
                                       "q", "sowas", "sowie", "spaeter", "web", "aotto", "bitten", "echt", "eigene", 
                                       "extra", "meint", "obwohl", "orf", "sei", "uro", "wen", "zb", "apa", "ber", "caf", 
                                       "card", "ch", "co", "gegenueber", "gern", "iv", "kleine", "more", "mt", "naechste", 
                                       "naemlich", "netzplanet", "se", "stream", "tops", "us", "waer", "we", "you", 
                                       "abendblatt", "aeh", "amepres", "cloud", "del", "deshalb", "gibts", "info", 
                                       "inzwischen", "irgendwie", "kennen", "klare", "nd", "palabra", "ohv", "schoene", 
                                       "server", "seien", "sog", "soviel", "svp", "swiss", "tolles", "zdfheute", "aha", 
                                       "al", "blzonline", "bot", "dji", "flue", "ger", "guten", "gutes", "jo", "me", 
                                       "migazin", "neben", "nen", "phoenix", "fuer", "koennen", "v", "di", "xd", "la", 
                              "tja", "hm", "klt", "hc", "hd", "az", "dlf", "ekd", "fr", "naja", "hey", "net", "orb", "spoe", 
                              "uk", "el", "sn", "daher", "immerhin", "le", "zumindest", "ah", "danach", "davor", "gvu", "juhu", 
                              "ka", "si", "wollt", "iii", "io", "od", "swr"))

c_stopwords <- c_stop %>%
  as_tibble() %>%
  bind_rows(stopwords)

# remove redundant data
rm(c_stop, stopwords)

# create token list 1764 different tokens
tweetwords <- tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(c_stopwords, by = "word") %>%
  count(word,  sort=T) %>%
  filter(n > 10) 
```


```{r}
# load sentiment tables from the Wortschatu by the University of Leipzig
sentiments <- bind_rows(
  read.table("SentiWS_v2.0_Negative.txt", sep = "\t", fill = T),
  read.table("SentiWS_v2.0_Positive.txt", sep = "\t", fill = T)) %>%
  select(.,1:2)

# add keywords because they aren't in the sentiment tables, 
# except "fluechtling" which has the sentiment value of -0.0048
# due to that I assign this value to the other keywords as well
f_pl <- data.frame(word = c("fluechtlinge", "migrant", "migranten", 
                            "asylant", "asylanten"), 
                   value = -0.0048)

key <- data.frame(word = c("islamisierung", "multikulti", "asyltouristen", 
                           "illegale", "merkel-gaeste", "zudringlinge", 
                           "buntland", "dummstaat", "plemplemland", 
                           "schandland", "bundeskloake", "wohlstandsfluechtlinge"),
                  value = -0.66)

key_slur <- data.frame(word = c("nafris", "musel", "salafistenschwestern", "kampfmuslimas",
                                "burka-frauen", "kloneger"),
                       value = -0.99)


# sentiments are transformed just as the tweets
# and a new dataset is created with expression which occurs in both (sentiment/tweet) datasets
sentis <- sentiments %>%
  mutate(word = str_replace_all(sentiments$V1,
                c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss", 
                  "Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
  rename("value" = "V2") %>%
  select(value, word) %>%
  mutate(word = str_to_lower(word)) %>%
  mutate(word = gsub("\\|nn", "", word)) %>%
  bind_rows(f_pl, key, key_slur) %>%
  inner_join(tweetwords, by = "word") %>%
  mutate(sentiment = ifelse(value >= 0, "positive", "negative")) %>%
  rename("freq" = "n") %>%
  arrange(desc(freq))

# calculate the overall positve and negative sentiments
sentis %>%
  group_by(sentiment) %>%
  count()

# remove redundant data
rm(f_pl, key, key_slur, sentiments)
```


```{r echo=FALSE, fig.height=5, fig.width=14, warning=FALSE}
# show the distribution of positive/negative seniments, proper

# replicaton note: to see this and the following plots within this file: comment out the jpeg and dev.off command
# otherwise the plots will be saved in your current woring directory

jpeg(file = "sentis.jpeg", width = 1000)

sentis %>%
  ggplot(aes(word, value, fill = sentiment, label = word)) +
  geom_col() +
  labs(title="Positive/Negative Sentiments of the Tweets", 
       y = "", x= "Tokens")  +
  coord_cartesian(ylim = c(-1,1)) +
  theme_minimal() +
  theme(axis.title.x = element_text(),
        axis.text.x = element_text(angle = 90),
        axis.ticks.x = element_blank(),
        axis.title.y = element_text(),
        axis.text.y = element_text(),
        axis.ticks.y = element_blank()) +
  guides(fill = guide_legend(title = "Sentiment"))

dev.off() 
```


```{r}
# reformat the date column to the class "date"
incidents$features_properties_date <- as.character(incidents$features_properties_date)
incidents$features_properties_date <- as.Date(incidents$features_properties_date, "%d.%m.%Y")

# cutting down the incidents dataset to the observations of interest (year 2015) 
incidents <- incidents %>%
  filter(features_properties_date >= "2015-01-01" & features_properties_date < "2016-01-01")
```

```{r}
# showing the distribution of the incidents against refugees within the year 2015 and the individual states
graph <- incidents %>%
  select(features_properties_city, features_properties_date, features_properties_state) %>%
  rename("city" = "features_properties_city", "date" = "features_properties_date", "state" = "features_properties_state") %>%
  group_by(state) %>%
  count()

ger <- readRDS("DEU_adm1.rds")
states <- fortify(ger)
states$state <- factor(as.numeric(states$id))
levels(states$state) <- ger$NAME_1
state_inc <- merge(states, graph)


jpeg(file = "refugee_dist.jpeg")

ggplot(state_inc, aes(x = long, y = lat, group = group, fill = n)) +
    geom_polygon(col = "white") +
    coord_map() +
    theme_void() +
    theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(title = "Incidents against Refugees in the Year 2015", y = "", x = "", fill = "Number of Incidents")

dev.off()
```


```{r}
# removing redundant data to unload the CPU
rm(ger, states, state_inc)

# encode the umlaute of the states to merge in the incident dataset
control$state <- str_replace_all(control$state,
                                 c("ue" = "ü"))
 
incidents <- incidents %>%
  select(features_properties_date, features_properties_state, features_properties_title) %>%
  rename("date" = "features_properties_date", "state" = "features_properties_state", "title" = "features_properties_title") %>%
  full_join(control, by = "state")

# cutting down the incidents dataset to the weeks of interest
inc_1 <- incidents %>%
  filter(date >= "2015-01-01" & date <= "2015-01-14")

inc_2 <- incidents %>%
  filter(date >= "2015-02-07" & date <= "2015-02-21")

inc_3 <- incidents %>%
  filter(date >= "2015-06-10" & date <= "2015-06-24")

inc_4 <- incidents %>%
  filter(date >= "2015-06-19" & date <= "2015-07-03")

inc_5 <- incidents %>%
  filter(date >= "2015-09-02" & date <= "2015-09-16")

inc_6 <- incidents %>%
  filter(date >= "2015-09-10" & date <= "2015-09-24")

inc_7 <- incidents %>%
  filter(date >= "2015-09-25" & date <= "2015-10-09")

inc_8 <- incidents %>%
  filter(date >= "2015-11-06" & date <= "2015-11-20")

inc_9 <- incidents %>%
  filter(date >= "2015-11-25" & date <= "2015-12-09")

incidents <- inc_1 %>%
  bind_rows(inc_2, inc_3, inc_4, inc_4, inc_5, inc_6, inc_7, inc_8, inc_9) %>%
  distinct()

# removing redundant data
rm(inc_1, inc_2, inc_3, inc_4, inc_5, inc_6, inc_7, inc_8, inc_9)

# reformat the date in the Twitter dataset to the class "date"
tweets$timestamp <- as.Date(tweets$timestamp, "%Y-%m-%d %H:%M:%S")

tweets <- tweets %>%
  select(tweet_id, timestamp, text) %>%
  rename("date" = "timestamp")
```


```{r}
# sum up the tweets per day
sum_tweets <- tweets %>%
  group_by(date) %>%
  count() %>%
  rename("tweets" = "n")

# sum up the incidents per day
sum_inc <- incidents %>%
  group_by(date) %>%
  count() %>%
  rename("num_inc" = "n")

# merge Tweets and incidents, adding wave indicators per treatment weeks, without missings
graph <- sum_tweets %>%
  inner_join(sum_inc, by = "date") %>%
  replace_na(list(tweets = 0, num_inc = 0)) %>%
  mutate(wave = if_else(date >= "2015-01-01" & date <= "2015-01-14", 1, 
                        if_else(date >= "2015-02-07" & date <= "2015-02-22", 2, 
                                if_else(date >= "2015-06-10" & date <= "2015-06-24", 3, 
                                        if_else(date >= "2015-06-19" & date <= "2015-07-04", 4,
                                                if_else(date >= "2015-09-02" & date <= "2015-09-16", 5,
                                                        if_else(date >= "2015-09-10" & date <= "2015-09-25", 6,
                                                                if_else(date >= "2015-09-25" & date <= "2015-10-10", 7, 
                                                                        if_else(date >= "2015-11-06" & date <= "2015-11-21", 8, 9)))))))))

# compute the natural logarithm to have a better visualization
sum_tweets[, 2] <- log(sum_tweets[2])
sum_inc[, 2] <- log(sum_inc[2])

# create the logarithmized dataset without missings, adding also wave indicators
graph_log <- sum_tweets %>%
  group_by(date) %>%
  inner_join(sum_inc, by = "date") %>%
  replace_na(list(tweets = 0, num_inc = 0)) %>%
  mutate(wave = if_else(date >= "2015-01-01" & date <= "2015-01-14", 1, 
                        if_else(date >= "2015-02-07" & date <= "2015-02-22", 2, 
                                if_else(date >= "2015-06-10" & date <= "2015-06-24", 3, 
                                        if_else(date >= "2015-06-19" & date <= "2015-07-04", 4,
                                                if_else(date >= "2015-09-02" & date <= "2015-09-16", 5,
                                                        if_else(date >= "2015-09-10" & date <= "2015-09-25", 6,
                                                                if_else(date >= "2015-09-25" & date <= "2015-10-10", 7, 
                                                                        if_else(date >= "2015-11-06" & date <= "2015-11-21", 8, 9)))))))))

# create name list for wave indicators, so the visualization is more meaningful
wave_names <- c("1" = "Week Jan",
                "2" = "Week Feb",
                "3" = "Week Jun",
                "4" = "Week Jun #2",
                "5" = "Week Sep",
                "6" = "Week Sep #2",
                "7" = "Week Oct",
                "8" = "Week Nov",
                "9" = "Week Dec")
```


```{r fig.height = 5, fig.width = 11}
# display the logarithmized graph
jpeg(file = "log_crime_tweets.jpeg")

graph_log %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = tweets, colour = "tweets")) +
  geom_line(aes(y = num_inc, colour = "num_inc")) +
  labs(title = "Logarithmized: Hate Crime and Tweets with the word 'refugee'", x = "Date", y = "Count", colour = "Legend") +
  scale_color_manual(labels = c("Number of Incidents", "Tweets"), values = c("#009E73", "#E69F00")) +
  theme_minimal() +
  facet_wrap(~ wave, scales = "free_x", labeller = as_labeller((wave_names)))

dev.off()
```


```{r fig.height = 5, fig.width = 11}
# display the original distribution
jpeg(file = "crime_tweets.jpeg")

graph %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = tweets, colour = "tweets")) +
  geom_line(aes(y = num_inc, colour = "num_inc")) +
  labs(title = "Hate Crime and Tweets with the word 'refugee'", x = "Date", y = "", colour = "Legend") +
  scale_color_manual(labels = c("Number of Incidents", "Tweets"), values = c("#009E73", "#E69F00")) +
  theme_minimal() +
  facet_wrap(~ wave, scales = "free_x", labeller = as_labeller(wave_names))

dev.off()
```


```{r}
# create another summed up Twitter dataset by date
sum_t1 <- tweets %>%
  group_by(date) %>%
  count() %>%
  rename("tweets" = "n")

# merge all data together with missings
all <- incidents %>%
  group_by(date, state) %>%
  count() %>%
  rename("num_inc" = "n") %>%
  full_join(sum_t1, by = "date") %>%
  full_join(control, by = "state") %>%
  replace_na(list(tweets = 0, num_inc = 0))

# compute statistics of summary
all1 <- incidents %>%
  group_by(date, state) %>%
  count() %>%
  rename("num_inc" = "n") %>%
  full_join(sum_t1, by = "date") %>%
  full_join(control, by = "state")

stats <- all1 %>%
  ungroup() %>%
  select(tweets, num_inc, hh_income_net, digital_infrastructure_16_perc)

stats1 <- all1 %>%
  ungroup() %>%
  select(population_density, foreigners.density, protection_seeking, racism_level_perc)

stats <- stat.desc(stats[, 1:4])
stats1 <- stat.desc(stats1[, 1:4])

# create Latex code for the summary statistics
print(xtable(stats, type = "latex"), file = "summarystats.txt")
print(xtable(stats1, type = "latex"), file = "summarystats1.txt")


all <- incidents %>%
  group_by(date, state) %>%
  count() %>%
  rename("num_inc" = "n") %>%
  inner_join(sum_t1, by = "date") %>%
  inner_join(control, by = "state") %>%
  replace_na(list(tweets = 0, num_inc = 0))

# remove redundant data
rm(c_stopwords, graph, graph_log, sum_inc, sum_tweets, tweetwords, sentis, sum_t1, control, wave_names, stats, stats1)


```


In the next few chunks the individual regressions around the nine treatmenst are calculated.

```{r}
inc_fra <- all %>%
  filter(date >= "2015-01-01" & date <= "2015-01-14") %>%
  mutate(before = if_else(date >= "2015-01-01" & date < "2015-01-07", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-01-07" & date < "2015-01-14", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-01-01" & date < "2015-01-07", 0, 1))

fra <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra)
#summary(fra)
```


```{r}
inc_dnk <- all %>%
  filter(date >= "2015-02-07" & date <= "2015-02-21") %>%
  mutate(before = if_else(date >= "2015-02-07" & date < "2015-02-14", mean(tweets), 0)) %>%
  mutate(after = if_else(date >= "2015-02-14" & date < "2015-02-21", mean(tweets), 0)) %>%
  mutate(treat = if_else(date >= "2015-02-07" & date < "2015-02-14", 0, 1))

dnk <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_dnk)
#summary(dnk)
```


```{r}
inc_usa1 <- all %>%
  filter(date >= "2015-06-10" & date <= "2015-06-24") %>%
  mutate(before = if_else(date >= "2015-06-10" & date < "2015-06-17", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-06-10" & date < "2015-06-24", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-06-10" & date < "2015-06-17", 0, 1))

usa1 <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_usa1)
#summary(usa1)
```


```{r}
inc_usa2 <- all %>%
  filter(date >= "2015-11-25" & date <= "2015-12-09") %>%
  mutate(before = if_else(date >= "2015-11-25" & date < "2015-12-02", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-11-25" & date <= "2015-12-09", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-11-25" & date < "2015-12-02", 0, 1))


usa2 <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_usa2)
#summary(usa2)
```


```{r}
inc_fra1 <- all %>%
  filter(date >= "2015-06-19" & date <= "2015-07-03") %>%
  mutate(before = if_else(date >= "2015-06-19" & date < "2015-06-26", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-06-19" & date <= "2015-07-03", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-06-19" & date < "2015-06-26", 0, 1))


fra1 <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra1)
#summary(fra1)
```

```{r}
inc_deu <- all %>%
  filter(date >= "2015-09-02" & date <= "2015-09-16") %>%
  mutate(before = if_else(date >= "2015-09-02" & date < "2015-09-09", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-09-02" & date <= "2015-09-16", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-09-02" & date < "2015-09-09", 0, 1))

deu <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_deu)
#summary(deu)
```


```{r}
inc_deu1 <- all %>%
  filter(date >= "2015-09-10" & date <= "2015-09-24") %>%
  mutate(before = if_else(date >= "2015-09-10" & date < "2015-09-17", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-09-10" & date <= "2015-09-24", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-09-10" & date < "2015-09-17", 0, 1))

deu1 <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_deu1)
#summary(deu1)
```


```{r}
inc_aus <- all %>%
  filter(date >= "2015-09-24" & date <= "2015-10-09") %>%
  mutate(before = if_else(date >= "2015-09-24" & date < "2015-10-02", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-09-24" & date <= "2015-10-09", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-09-24" & date < "2015-10-02", 0, 1))

aus <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_aus)
#summary(aus)
```


```{r}
inc_fra2 <- all %>%
  filter(date >= "2015-11-06" & date <= "2015-11-20") %>%
  mutate(before = if_else(date >= "2015-11-06" & date < "2015-11-13", tweets*1, 0)) %>%
  mutate(after = if_else(date >= "2015-11-06" & date <= "2015-11-20", tweets*1, 0)) %>%
  mutate(treat = if_else(date >= "2015-11-06" & date < "2015-11-13", 0, 1))

fra2 <- lm((after - before) ~ treat + num_inc + population_density + foreigners.density + protection_seeking + 
          racism_level_perc + hh_income_net + digital_infrastructure_16_perc, data = inc_fra2)
#summary(fra2)
```


```{r}
# create Latex code from the regression outputs, all treatments in France
stargazer(fra, fra1, fra2, title = "Regression Results France",
          dep.var.labels = c("Tweets"),
          covariate.labels = c("Treatment", "Number of Incidents", "Population Density", "Foreigner Density", "People who are seeking for protection",
                               "Racism Level in Percentage", "Houshold Income (Net)", "Digital Infrastructure"),
          omit.stat = c("LL", "ser", "f"), ci = F, single.row = T)
```


```{r}
# create Latex code from the regression outputs, treatments in Denmark and Germany
stargazer(dnk, deu, deu1, title = "Regression Results Europe (Denmark and Germany)",
          dep.var.labels = c("Tweets"),
          covariate.labels = c("Treatment", "Number of Incidents", "Population Density", "Foreigner Density", "People who are seeking for protection",
                               "Racism Level in Percentage", "Houshold Income (Net)", "Digital Infrastructure"),
          omit.stat = c("LL", "ser", "f"), ci = F, single.row = T)
```



```{r}
# create Latex code from regression outputs, transoceanic treatments USA and Australia
stargazer(usa1, usa2, aus, title = "Regression Results USA and Australia",
          dep.var.labels = c("Tweets"),
          covariate.labels = c("Treatment", "Number of Incidents", "Population Density", "Foreigner Density", "People who are seeking for protection",
                               "Racism Level in Percentage", "Houshold Income (Net)", "Digital Infrastructure"),
          omit.stat = c("LL", "ser", "f"), ci = F, single.row = T)
```



