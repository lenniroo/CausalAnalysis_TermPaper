# remove the URLs
rm(inc)
rm(first)
rm(fl)
rm(asy)
rm(mi)
# compute the difference between the first and the second scrape process
diff_firstsecond_scrape <- tweets_first %>%
anti_join(fl_tweets, by = "text")
# binding all tweets together
tweets <- fl_tweets %>%
bind_rows(diff_firstsecond_scrape) %>%
bind_rows(asy_tweets) %>%
bind_rows(mi_tweets)
# remove redundant dataframes of single scrape waves
rm(tweets_first)
rm(fl_tweets)
rm(asy_tweets)
rm(mi_tweets)
rm(diff_firstsecond_scrape)
# encode all tweets to utf-8
tweets$text <- stri_encode(tweets$text, "", "UTF-8")
# transform tweets to a corpus to remove URLs within tweets
tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))
#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
# removing URLs
corpus <- tm_map(corpus, removeURL)
# transfomr corpus to dataframe and replace the tweets within the tweet data
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)
tweets$text <- d$text
# remove redundant data
rm(d)
rm(tweetdata)
rm(corpus)
# transform to tibble to work the tidyverse package
tweets <- as_tibble(tweets)
class(tweets$text)
# transform factors to characters, easier to handle
tweets %>%
mutate_if(is.factor, as.character)
# transform umlauts and remove all non-alphanumerical characters
tweets$text <- str_replace_all(tweets$text,
c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss",
"Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue",
"Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue", "ÃŸ" = "ss",
"Ã„" = "Ae", "Ã–" = "Oe", "Ãœ" = "Ue", "[\r\n]" = " ",
"[[:punct:]]" = " " , "[^[:alnum:] ]" = " ",
"[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " "))
tweets$text <- tolower(tweets$text)
# delete non german or englisch tweets during removing unicode specific tweets
# (contains \s + hexdecimal numbers)
# which cannot be transformed to utf-8
setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]
empty_char <- tweets %>%
filter(!str_detect(text, ""))
tweets <- tweets %>%
anti_join(empty_char, by = "text")
rm(empty_char)
# loading stopwords and transform them equally to the tweet transformation
stopwords <- data_frame(word = stopwords("de"))
stopwords <- stopwords %>%
data_frame(str_replace_all(stopwords$word,
c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss",
"Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
set_colnames(c("word1", "word2")) %>%
select("word2") %>%
rename("word" = "word2")
# due to the fact that the loaded german stopwords doesn't contain web expressions like "wtf", "lol", etc.
# these expression are added manually to the stopword list
c_stop <- data.frame(word = c("via", "mehr", "twitter", "com", "pic", "de", "eu", "d", "kommen", "mal",
"sollen", "news", "viele", "ja", "online", "geht", "heute", "schon", "neue",
"u", "immer", "muessen", "wer", "gibt", "s", "ab", "rt", "gut", "macht",
"statt", "a", "n", "nimmt", "sagt", "seit", "unsere", "fast", "sz", "f",
"brauchen", "beim", "nehmen", "neue", "bitte", "gerade", "zurueck", "is",
"lassen", "dafuer", "einfach", "wegen", "f", "m", "kommt", "denen", "re",
"duerfen", "i", "wohl", "waere", "weitere", "b", "gehen", "gar", "erst", "erste",
"sollten", "wirklich", "h", "sagen", "sieht", "bringen", "eigentlich",
"suchen", "gg", "o", "tun", "ganz", "stellt", "e", "laesst", "w", "bald",
"br", "faz", "faznet", "at", "geben", "ndr", "no", "t", "z", "all", "for",
"g", "davon", "haette", "wurde", "setzt", "the", "genau", "k", "r", "gleich",
"kamen", "c", "koennten", "l", "waeren", "ntvde", "vielleicht", "dabei",
"darf", "haelt", "offenbar", "wenig", "allein", "kaum", "orfsg", "by", "klar",
"wg", "bislang", "na", "neuen", "eignen", "heisst", "leider", "letzte",
"paar", "ots", "womoeglich", "add", "etc", "ne", "sogar", "trotz", "schnell",
"wdr", "jemand", "tolle", "wieso", "boah", "ganze", "halten", "rp", "darum",
"hh", "toll", "top", "x", "bereits", "etwa", "schoen", "taz", "domain", "her",
"j", "mdr", "orban", "p", "sv", "tt", "wollten", "ex", "focusonline", "live",
"nix", "nzz", "of", "tut", "ca", "ikea", "mrd", "unsere", "alte", "daran",
"darueber", "deutlich", "eher", "ha", "je", "laedt", "laesst", "moechte",
"stimmt", "ueberall", "usw", "telmi", "zib", "ach", "and", "eben", "en",
"natuerlich", "to", "ttip", "alter", "egal", "fb", "fc", "heutejournal",
"oft", "sternde", "vs", "aktuelle" ,"au", "bisserl", "gerne", "grosses",
"guter", "here", "magazin", "oh", "sant", "st", "till", "srfarena", "deren",
"ey", "gab", "ganzen", "haetten", "heuteshow", "per", "wann", "ardde", "deai",
"dr", "hast", "kurz", "on", "rponline", "somit", "telekom", "tonline", "unseren",
"zusammen", "art", "ghadajreidi", "hna", "mag", "mo", "oevp", "sc", "selber",
"whataretheodds", "weder", "wieviele", "worden", "y", "be", "bplus", "gilt",
"it", "kam", "seid", "serie", "steve", "ts", "uebringens", "unseren", "wooow",
"binnen", "darauf", "daserste", "dwn", "fl", "fpoe", "free", "habt", "los", "mass",
"nahm", "newsrepublicde", "oe", "sst", "staendig", "weist", "weit", "wtf",
"badlands", "bvbqfk", "deut", "drk", "dt", "eh", "hallo", "halt", "lt", "new",
"q", "sowas", "sowie", "spaeter", "web", "aotto", "bitten", "echt", "eigene",
"extra", "meint", "obwohl", "orf", "sei", "uro", "wen", "zb", "apa", "ber", "caf",
"card", "ch", "co", "gegenueber", "gern", "iv", "kleine", "more", "mt", "naechste",
"naemlich", "netzplanet", "se", "stream", "tops", "us", "waer", "we", "you",
"abendblatt", "aeh", "amepres", "cloud", "del", "deshalb", "gibts", "info",
"inzwischen", "irgendwie", "kennen", "klare", "nd", "palabra", "ohv", "schoene",
"server", "seien", "sog", "soviel", "svp", "swiss", "tolles", "zdfheute", "aha",
"al", "blzonline", "bot", "dji", "flue", "ger", "guten", "gutes", "jo", "me",
"migazin", "neben", "nen", "phoenix", "sn", "vds"))
c_stopwords <- c_stop %>%
as_tibble() %>%
bind_rows(stopwords)
# remove redundant data
rm(c_stop)
rm(stopwords)
# create token list 1755 different tokens
tweetwords <- tweets %>%
unnest_tokens(word, text) %>%
anti_join(c_stopwords, by = "word") %>%
count(word,  sort=T) %>%
filter(n > 10) #more or less clean dataset with 1503 tokens
# load sentiment tables from the Wortschatu by the University of Leipzig
sentiments <- bind_rows(
read.table("SentiWS_v2.0_Negative.txt", sep = "\t", fill = T),
read.table("SentiWS_v2.0_Positive.txt", sep = "\t", fill = T)) %>%
select(.,1:2)
# add keywords because they aren't in the sentiment tables,
# except "fluechtling" which has the sentiment value of -0.0048
# due to that I assign this value to the other keywords as well
f_pl <- data.frame(word = c("fluechtlinge", "migrant", "migranten",
"asylant", "asylanten"),
value = -0.0048)
# sentiments are transformed just as the tweets
# and a new dataset is created with expression which occurs in both (sentiment/tweet) datasets
sentis <- sentiments %>%
mutate(word = str_replace_all(sentiments$V1,
c("Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue", "ÃŸ" = "ss",
"Ã„" = "ae", "Ã–" = "Oe", "Ãœ" = "Ue"))) %>%
rename("value" = "V2") %>%
select(value, word) %>%
mutate(word = str_to_lower(word)) %>%
mutate(word = gsub("\\|nn", "", word)) %>%
bind_rows(f_pl) %>%
inner_join(tweetwords, by = "word") %>%
mutate(sentiment = ifelse(value >= 0, "positive", "negative")) %>%
rename("freq" = "n") %>%
arrange(desc(freq))
# remove redundant data
rm(f_pl)
rm(sentiments)
# calculate the overall positve and negative sentiments
sentis %>%
group_by(sentiment) %>%
count()
# show the distribution of positive/negative seniments, proper
sentis %>%
#filter(freq > 20) %>%
ggplot(aes(word, value, fill = sentiment, label = word)) +
geom_col() +
#geom_text(aes(y = 0), angle = 90, hjust = -.075, vjust = .1, size = 3) +
labs(title="Positive/Negative Sentiments of the Tweets",
y = "", x= "Tokens")  +
coord_cartesian(ylim = c(-1,1)) +
theme_minimal() +
theme(axis.title.x = element_text(),
axis.text.x = element_text(angle = 90),
axis.ticks.x = element_blank(),
axis.title.y = element_text(),
axis.text.y = element_text(),
axis.ticks.y = element_blank()) +
guides(fill = guide_legend(title = "Sentiment"))
incidents$features_properties_date <- as.character(incidents$features_properties_date)
incidents$features_properties_date <- as.Date(incidents$features_properties_date, "%d.%m.%Y")
incidents <- incidents %>%
filter(features_properties_date >= "2015-01-01" & features_properties_date < "2016-01-01")
graph <- incidents %>%
select(features_properties_city, features_properties_date, features_properties_state) %>%
rename("city" = "features_properties_city", "date" = "features_properties_date", "state" = "features_properties_state") %>%
group_by(state) %>%
count()
ger <- readRDS("DEU_adm1.rds")
states <- fortify(ger)
states$state <- factor(as.numeric(states$id))
levels(states$state) <- ger$NAME_1
state_inc <- merge(states, graph)
ggplot(state_inc, aes(x = long, y = lat, group = group, fill = n)) +
geom_polygon(col = "white") +
coord_map() +
theme_void() +
theme(axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(title = "Incidents against Refugees in the Year 2015", y = "", x = "", fill = "Number of Incidents")
incidents <- incidents %>%
select(features_properties_date, features_properties_state, features_properties_title) %>%
rename("date" = "features_properties_date", "state" = "features_properties_state", "title" = "features_properties_title")
tweets <- tweets %>%
select(tweet_id, timestamp, text)
#loading required packages, it downloads a package if it isn't downloaded yet
if (!require(RCurl)) install.packages("RCurl", dependencies = T)
library(RCurl)
if (!require(curl)) install.packages("curl", dependencies = T)
library(curl)
if (!require(tidyverse)) install.packages("tidyverse", dependencies = T)
library(tidyverse)
if (!require(tidytext)) install.packages("tidytext", dependencies = T)
library(tidytext)
if (!require(stringi)) install.packages("stringi", dependencies = T)
library(stringi)
if (!require(quanteda)) install.packages("quanteda", dependencies = T)
library(quanteda)
if (!require(tm)) install.packages("tm", dependencies = T)
library(tm)
if (!require(data.table)) install.packages("data.table", dependencies = T)
library(data.table)
if (!require(tokenizers)) install.packages("tokenizers", dependencies = T)
library(tokenizers)
if (!require(magrittr)) install.packages("magrittr", dependencies = T)
library(magrittr)
if (!require(anytime)) install.packages("anytime", dependencies = T)
library(anytime)
if (!require(rgdal)) install.packages("rgdal", dependencies = T)
library(rgdal)
if (!require(sp)) install.packages("sp")
library(sp)
if (!require(mapproj)) install.packages("mapproj")
library(mapproj)
# loading Data
# incident data provided by Mueller and Schwarz (2019)
inc <- getURL("https://raw.githubusercontent.com/ax3l/chronik-vorfaelle/data/vorfaelle.csv")
incidents <- read.csv(text = inc)
# first scrape of the keyword "Fluechtling(e)" (refugee(s))
first <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_first.csv")
tweets_first <- read.csv(text = first)
# second scrape of "Fluechtling(e)" (refugee(s))
fl <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_f.csv")
fl_tweets <- read.csv(text = fl)
# scrape of the keyword "asylant" (asylee(s))
asy <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_a.csv")
asy_tweets <- read.csv(text = asy)
# scrape of the keyword "migrant(en)" (migrant(s))
mi <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/Scrape_m.csv")
mi_tweets <- read.csv(text = mi)
# remove the URLs
rm(inc)
rm(first)
rm(fl)
rm(asy)
rm(mi)
# compute the difference between the first and the second scrape process
diff_firstsecond_scrape <- tweets_first %>%
anti_join(fl_tweets, by = "text")
# binding all tweets together
tweets <- fl_tweets %>%
bind_rows(diff_firstsecond_scrape) %>%
bind_rows(asy_tweets) %>%
bind_rows(mi_tweets)
# remove redundant dataframes of single scrape waves
rm(tweets_first)
rm(fl_tweets)
rm(asy_tweets)
rm(mi_tweets)
rm(diff_firstsecond_scrape)
# encode all tweets to utf-8
tweets$text <- stri_encode(tweets$text, "", "UTF-8")
# transform tweets to a corpus to remove URLs within tweets
tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))
#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
# removing URLs
corpus <- tm_map(corpus, removeURL)
# transfomr corpus to dataframe and replace the tweets within the tweet data
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)
tweets$text <- d$text
# remove redundant data
rm(d)
rm(tweetdata)
rm(corpus)
# transform to tibble to work the tidyverse package
tweets <- as_tibble(tweets)
class(tweets$text)
# transform factors to characters, easier to handle
tweets %>%
mutate_if(is.factor, as.character)
# transform umlauts and remove all non-alphanumerical characters
tweets$text <- str_replace_all(tweets$text,
c("Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue", "Ã" = "ss",
"Ã" = "Ae", "Ã" = "Oe", "Ã" = "Ue",
"ÃÂ¤" = "ae", "ÃÂ¶" = "oe", "ÃÂ¼" = "ue", "ÃÅ¸" = "ss",
"Ãâ" = "Ae", "Ãâ" = "Oe", "ÃÅ" = "Ue", "[\r\n]" = " ",
"[[:punct:]]" = " " , "[^[:alnum:] ]" = " ",
"[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " "))
tweets$text <- tolower(tweets$text)
# delete non german or englisch tweets during removing unicode specific tweets
# (contains \s + hexdecimal numbers)
# which cannot be transformed to utf-8
setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]
empty_char <- tweets %>%
filter(!str_detect(text, ""))
tweets <- tweets %>%
anti_join(empty_char, by = "text")
rm(empty_char)
# transform to tibble to work the tidyverse package
tweets <- as_tibble(tweets)
class(tweets$text)
# transform factors to characters, easier to handle
tweets %>%
mutate_if(is.factor, as.character)
# transform umlauts and remove all non-alphanumerical characters
tweets$text <- str_replace_all(tweets$text,
c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss",
"Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue",
"Â¤" = "ae", "Â¶" = "oe", "Â¼" = "ue", "ÃŸ" = "ss",
"Ã„" = "Ae", "Ã–" = "Oe", "Ãœ" = "Ue", "[\r\n]" = " ",
"[[:punct:]]" = " " , "[^[:alnum:] ]" = " ",
"[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " "))
tweets$text <- tolower(tweets$text)
# loading stopwords and transform them equally to the tweet transformation
stopwords <- data_frame(word = stopwords("de"))
stopwords <- stopwords %>%
data_frame(str_replace_all(stopwords$word,
c("Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue", "ß" = "ss",
"Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
set_colnames(c("word1", "word2")) %>%
select("word2") %>%
rename("word" = "word2")
# due to the fact that the loaded german stopwords doesn't contain web expressions like "wtf", "lol", etc.
# these expression are added manually to the stopword list
c_stop <- data.frame(word = c("via", "mehr", "twitter", "com", "pic", "de", "eu", "d", "kommen", "mal",
"sollen", "news", "viele", "ja", "online", "geht", "heute", "schon", "neue",
"u", "immer", "muessen", "wer", "gibt", "s", "ab", "rt", "gut", "macht",
"statt", "a", "n", "nimmt", "sagt", "seit", "unsere", "fast", "sz", "f",
"brauchen", "beim", "nehmen", "neue", "bitte", "gerade", "zurueck", "is",
"lassen", "dafuer", "einfach", "wegen", "f", "m", "kommt", "denen", "re",
"duerfen", "i", "wohl", "waere", "weitere", "b", "gehen", "gar", "erst", "erste",
"sollten", "wirklich", "h", "sagen", "sieht", "bringen", "eigentlich",
"suchen", "gg", "o", "tun", "ganz", "stellt", "e", "laesst", "w", "bald",
"br", "faz", "faznet", "at", "geben", "ndr", "no", "t", "z", "all", "for",
"g", "davon", "haette", "wurde", "setzt", "the", "genau", "k", "r", "gleich",
"kamen", "c", "koennten", "l", "waeren", "ntvde", "vielleicht", "dabei",
"darf", "haelt", "offenbar", "wenig", "allein", "kaum", "orfsg", "by", "klar",
"wg", "bislang", "na", "neuen", "eignen", "heisst", "leider", "letzte",
"paar", "ots", "womoeglich", "add", "etc", "ne", "sogar", "trotz", "schnell",
"wdr", "jemand", "tolle", "wieso", "boah", "ganze", "halten", "rp", "darum",
"hh", "toll", "top", "x", "bereits", "etwa", "schoen", "taz", "domain", "her",
"j", "mdr", "orban", "p", "sv", "tt", "wollten", "ex", "focusonline", "live",
"nix", "nzz", "of", "tut", "ca", "ikea", "mrd", "unsere", "alte", "daran",
"darueber", "deutlich", "eher", "ha", "je", "laedt", "laesst", "moechte",
"stimmt", "ueberall", "usw", "telmi", "zib", "ach", "and", "eben", "en",
"natuerlich", "to", "ttip", "alter", "egal", "fb", "fc", "heutejournal",
"oft", "sternde", "vs", "aktuelle" ,"au", "bisserl", "gerne", "grosses",
"guter", "here", "magazin", "oh", "sant", "st", "till", "srfarena", "deren",
"ey", "gab", "ganzen", "haetten", "heuteshow", "per", "wann", "ardde", "deai",
"dr", "hast", "kurz", "on", "rponline", "somit", "telekom", "tonline", "unseren",
"zusammen", "art", "ghadajreidi", "hna", "mag", "mo", "oevp", "sc", "selber",
"whataretheodds", "weder", "wieviele", "worden", "y", "be", "bplus", "gilt",
"it", "kam", "seid", "serie", "steve", "ts", "uebringens", "unseren", "wooow",
"binnen", "darauf", "daserste", "dwn", "fl", "fpoe", "free", "habt", "los", "mass",
"nahm", "newsrepublicde", "oe", "sst", "staendig", "weist", "weit", "wtf",
"badlands", "bvbqfk", "deut", "drk", "dt", "eh", "hallo", "halt", "lt", "new",
"q", "sowas", "sowie", "spaeter", "web", "aotto", "bitten", "echt", "eigene",
"extra", "meint", "obwohl", "orf", "sei", "uro", "wen", "zb", "apa", "ber", "caf",
"card", "ch", "co", "gegenueber", "gern", "iv", "kleine", "more", "mt", "naechste",
"naemlich", "netzplanet", "se", "stream", "tops", "us", "waer", "we", "you",
"abendblatt", "aeh", "amepres", "cloud", "del", "deshalb", "gibts", "info",
"inzwischen", "irgendwie", "kennen", "klare", "nd", "palabra", "ohv", "schoene",
"server", "seien", "sog", "soviel", "svp", "swiss", "tolles", "zdfheute", "aha",
"al", "blzonline", "bot", "dji", "flue", "ger", "guten", "gutes", "jo", "me",
"migazin", "neben", "nen", "phoenix", "sn", "vds"))
c_stopwords <- c_stop %>%
as_tibble() %>%
bind_rows(stopwords)
# remove redundant data
rm(c_stop)
rm(stopwords)
# create token list 1755 different tokens
tweetwords <- tweets %>%
unnest_tokens(word, text) %>%
anti_join(c_stopwords, by = "word") %>%
count(word,  sort=T) %>%
filter(n > 10) #more or less clean dataset with 1503 tokens
# load sentiment tables from the Wortschatu by the University of Leipzig
sentiments <- bind_rows(
read.table("SentiWS_v2.0_Negative.txt", sep = "\t", fill = T),
read.table("SentiWS_v2.0_Positive.txt", sep = "\t", fill = T)) %>%
select(.,1:2)
# add keywords because they aren't in the sentiment tables,
# except "fluechtling" which has the sentiment value of -0.0048
# due to that I assign this value to the other keywords as well
f_pl <- data.frame(word = c("fluechtlinge", "migrant", "migranten",
"asylant", "asylanten"),
value = -0.0048)
# sentiments are transformed just as the tweets
# and a new dataset is created with expression which occurs in both (sentiment/tweet) datasets
sentis <- sentiments %>%
mutate(word = str_replace_all(sentiments$V1,
c("ä" = "ae", "ö" = "oe", "ü" = "ue", "ß" = "ss",
"Ä" = "Ae", "Ö" = "Oe", "Ü" = "Ue"))) %>%
rename("value" = "V2") %>%
select(value, word) %>%
mutate(word = str_to_lower(word)) %>%
mutate(word = gsub("\\|nn", "", word)) %>%
bind_rows(f_pl) %>%
inner_join(tweetwords, by = "word") %>%
mutate(sentiment = ifelse(value >= 0, "positive", "negative")) %>%
rename("freq" = "n") %>%
arrange(desc(freq))
# remove redundant data
rm(f_pl)
rm(sentiments)
# calculate the overall positve and negative sentiments
sentis %>%
group_by(sentiment) %>%
count()
# show the distribution of positive/negative seniments, proper
sentis %>%
ggplot(aes(word, value, fill = sentiment, label = word)) +
geom_col() +
labs(title="Positive/Negative Sentiments of the Tweets",
y = "", x= "Tokens")  +
coord_cartesian(ylim = c(-1,1)) +
theme_minimal() +
theme(axis.title.x = element_text(),
axis.text.x = element_text(angle = 90),
axis.ticks.x = element_blank(),
axis.title.y = element_text(),
axis.text.y = element_text(),
axis.ticks.y = element_blank()) +
guides(fill = guide_legend(title = "Sentiment"))
incidents$features_properties_date <- as.character(incidents$features_properties_date)
incidents$features_properties_date <- as.Date(incidents$features_properties_date, "%d.%m.%Y")
incidents <- incidents %>%
filter(features_properties_date >= "2015-01-01" & features_properties_date < "2016-01-01")
graph <- incidents %>%
select(features_properties_city, features_properties_date, features_properties_state) %>%
rename("city" = "features_properties_city", "date" = "features_properties_date", "state" = "features_properties_state") %>%
group_by(state) %>%
count()
ger <- readRDS("DEU_adm1.rds")
states <- fortify(ger)
states$state <- factor(as.numeric(states$id))
levels(states$state) <- ger$NAME_1
state_inc <- merge(states, graph)
ggplot(state_inc, aes(x = long, y = lat, group = group, fill = n)) +
geom_polygon(col = "white") +
coord_map() +
theme_void() +
theme(axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(title = "Incidents against Refugees in the Year 2015", y = "", x = "", fill = "Number of Incidents")
rm(ger)
rm(states)
rm(state_inc)
incidents <- incidents %>%
select(features_properties_date, features_properties_state, features_properties_title) %>%
rename("date" = "features_properties_date", "state" = "features_properties_state", "title" = "features_properties_title")
tweets <- tweets %>%
select(tweet_id, timestamp, text)
View(incidents)
View(graph)
c <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/control_variables.csv")
control <- read.csv(text = c)
View(control)
View(control)
View(incidents)
control$state <- str_replace_all(control$state,
c("<fc>" = "ü"))
View(control)
control$state <- str_replace_all(control$state,
c("<fc>" = "ue"))
View(control)
control$state <- str_replace_all(control$state,
c("<U+FFFD>" = "ue"))
View(control)
c <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/control_variables.csv")
control <- read.csv(text = c)
control$state <- stri_encode(control$state, "", "UTF-8")
View(control)
control$state <- stri_replace_all(control$state,
c("<U+FFFD>" = "ü"))
control$state <- str_replace_all(control$state,
c("<U+FFFD>" = "ü"))
View(control)
c <- getURL("https://raw.githubusercontent.com/lenniroo/CausalAnalysis_TermPaper/master/control_variables.csv")
control <- read.csv(text = c)
View(control)
iconvlist(c)
iconvlist(control)
iconvlist()
encoding(control)
Encoding(control)
Encoding(control) <- "UTF-8"
cat(control)
class(control)
class(control)
